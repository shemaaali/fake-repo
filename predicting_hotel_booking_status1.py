# -*- coding: utf-8 -*-
"""Predicting_Hotel_Booking_Status1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yqdQ1WeGD1p7YxvsMPu_KnrN6LK5UTG5
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import sys
#  
# # If you're on Colab:
# if 'google.colab' in sys.modules:
#     url_train = 'https://raw.githubusercontent.com/oyrx/PHBS_MLF_2019_Project/master/data/train.csv'
#     !pip install category_encoders==2.*
#     !pip install pandas-profiling==2.*
#     !pip install eli5
#     !pip install pdpbox
#     !pip install shap
#     !pip install scikitplot

# Some imports setup for the environment for python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
 
# Some imports setup for the environment for machine learnig
from sklearn.model_selection import KFold, cross_validate, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import RandomizedSearchCV

# Loading the dataset file and read it
url_hotel = 'https://raw.githubusercontent.com/oyrx/PHBS_MLF_2019_Project/master/data/hotel_bookings.csv'

hotel_booked = pd.read_csv(url_hotel)

print(hotel_booked.shape)
hotel_booked.head()

"""### Choosing The Target That I am Going To Predict

### Which column in my tabular dataset will I predict?
"""

# some description about reservation_status
hotel_booked['reservation_status'].describe()

# Derive a binary classification target
# Drope all unuseful hotel booking
hotel_booked['reservation_status']
# replace N-Show status canceled status(added to Canceled)
hotel_booked['reservation_status'] = hotel_booked['reservation_status'].replace({'No-Show':'Canceled'})
hotel_booked = hotel_booked.dropna(subset=['reservation_status'])
hotel_booked['reservation_status'] = hotel_booked['reservation_status'].apply(lambda x: 0 if x =='Canceled' else 1)

"""### How is my target distributed?"""

# Define a status hotel booking as having reservation_status of cheked out, canceled, or no show
hotel_booked['reservation_status'].describe()

# plot the distribution for reservation_status rating of 0 lower and 1 as higher
plt.figure(figsize= (10,8))
sns.distplot(hotel_booked['reservation_status'], color='red');

"""### My target is a classification problem:
I have derived my target as a binary classification problem:to answer a binary questions whether the booking_status checked out:1 or canceled:0

### How many classes?
"""

# I can assign my target to a variable is called my_target
my_target = hotel_booked['reservation_status']

# check for the first 10 reservation_status in my_target
my_target.head(10)

# how many numbers of nunique values are in my target column?
my_target.nunique()

"""### There are two class, and they are binary classification problems

### Are the classes imbalanced?
"""

# value counts for each class
my_target.value_counts(normalize=True)

# the max value counts
my_target.value_counts(normalize=True).max()

"""### Sometimes, the majority class occurs with less than 50% frequency which can be a mis-leading. In this case, if it happens, accuracy is not good as an evaluation metrix.
For my target the majority class occurs with 62% frequency, so I gues this is not too imbalanced. I could just use accuracy score as my evaluation metric if I want to deep more there are other options like precision, recall, and roc-auc. Sometimes, the majority class occurs with less than 50% frequency which can be a mis-leading. In this case, if it happens, accuracy is not good.

### Choosing My Evaluation Metrixes
In addtion to Accuracy, I am going to use Precision and Recall, ROC_curve/AUC, Confusion-Metrix, and Classification Report.

### Begin With Cleaning Data and Exploratory Data Analysis

### Import Wrangle
"""

# some information about the dataset
hotel_booked.info()

# some description about the dataset
hotel_booked.describe()

# checking for any missing values
hotel_booked.isnull().sum()

"""### Exploratory And Data Analysis(EDA)"""

from pandas_profiling import ProfileReport
# pull the report profile for my dataset
profile = ProfileReport(hotel_booked, minimal=True).to_notebook_iframe()

# use a seaborn countplot to plot reservation_status
plt.figure(figsize= (11,7))
ax = sns.countplot(x = "reservation_status", data = hotel_booked, color='brown', saturation=2, dodge=True);
sns.set(style = "darkgrid");
plt.title("Reservation Status Wether Checked-Out Or Canceled", fontdict = {'fontsize': 15});

# make a density curve of lead time per booking_status
lead_booking = (sns.FacetGrid(hotel_booked, hue='reservation_status', height=8, xlim=(0,400), legend_out=True).map(sns.kdeplot, 'lead_time', shade=True).add_legend());

# it shows the difference of booking cancellations for th total amount of book canceled or not and the percentage(resort and city hotels)
resort_hotel = hotel_booked[hotel_booked["hotel"] == "Resort Hotel"]
city_hotel = hotel_booked[hotel_booked["hotel"] == "City Hotel"]

cancel_reso = pd.DataFrame(resort_hotel["is_canceled"].value_counts())
cancel_reso.rename(columns={"is_canceled": "booking_cancellations"}, index =({0: "No Canceled", 1:"Yes Canceled"}), inplace=True)
cancel_reso["Status"] = cancel_reso.index

cancel_city = pd.DataFrame(city_hotel["is_canceled"].value_counts())
cancel_city.rename(columns={"is_canceled": "booking_cancellations"}, index =({0: "No Canceled", 1:"Yes Canceled"}), inplace=True)
cancel_city["Status"] = cancel_city.index

# make a pie chart to show the differece book canceled for those hotels

fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]], subplot_titles = ["City Hotel", "Resort Hotel"])
fig.add_trace(go.Pie(values = cancel_city["booking_cancellations"], labels = cancel_city["Status"]),1,1)
fig.add_trace(go.Pie(values = cancel_reso["booking_cancellations"], labels = cancel_reso["Status"]),1,2)
fig.update_traces(textposition='inside', textinfo='value+percent+label')
fig.update_layout(title_text = "Type Of Booking Cancellations For City Hotel And Resort Hotel")
fig.show()

# scatter plot between reservation_status and lead_time
px.scatter(hotel_booked, x='reservation_status', y='lead_time', color='reservation_status', opacity=0.1)

# add new feature engineering from existing adr column
# make a new column called total overnight cost in each hotel for guest
hotel_booked["adr_pp"] =hotel_booked["adr"] / (hotel_booked["adults"] + hotel_booked["children"])
room_guest = hotel_booked[hotel_booked["reservation_status"]== 0]
room_cost = room_guest[["hotel", "adr_pp", "reserved_room_type"]].sort_values("reserved_room_type")

plt.figure(figsize= (13,9))
sns.boxplot(x="reserved_room_type", y= "adr_pp", hue="hotel", data= room_cost, fliersize=0, color='green', saturation=0.70, width=0.8, dodge=True )
plt.ylim(0,170)

hotel_booked.head()

# countries that show the most total number of guests'homes for each hotel 
country_guest = pd.DataFrame(hotel_booked.loc[hotel_booked['is_repeated_guest'] == 0]["country"].value_counts())
country_guest.index.name = "country"
country_guest.rename(columns={"country": "Number of Guests"}, inplace=True)
total_num_guests = country_guest["Number of Guests"].sum()
country_guest["% Guests"] = round(country_guest["Number of Guests"] / total_num_guests * 100, 2)

# country per guest shows on the map
guest_per_country = px.choropleth(country_guest,
                    locations=country_guest.index,
                    color=country_guest["% Guests"], 
                    hover_name=country_guest.index, 
                    color_continuous_scale=px.colors.sequential.Plasma,
                    title="The Majority Countries That Guests Come From")
guest_per_country.show()

"""### My Wrangler Data Function
I have defined a function do all cleaning data and return my_wrangler_data
"""

# define a wrangle function to deal with training, validation, and testing data
# Do some cleaning and making feature engineering
import numpy as np

def my_wrangler_data(X):
    
    # SettingWithCopyWarning
    X = X.copy()


    # replace those columns that have missing value and drop the subset of children
    null_values = {"country": "Unknown", "agent": 0, "company": 0}
    X = X.fillna(null_values)
    X.dropna(subset = ["children"], inplace = True)

    # droping all invalids entries for adults, children, babies, weekened-nights, week-night, and adr
    X.drop(X[(X["adults"] == 0) & (X["children"] == 0) & (X["babies"] == 0)].index, inplace = True)
    X.drop(X[(X["stays_in_weekend_nights"] == 0) & (X["stays_in_weekend_nights"] == 0) & (X["adr"] == 0)].index, inplace = True)
    

    # Ajdusting the some names of columns in the dataset
    X.rename(columns = {"is_canceled": "booking_cancellations","arrival_date_year":"year", "arrival_date_month":"month", "arrival_date_week_number": "week_number", "arrival_date_day_of_month":"day_of_month", "stays_in_weekend_nights":"weekend_nights", "stays_in_week_nights":"week_nights"}, inplace=True)
     

    # the different between booking cancellations in City Hotel and Resort Hotel
    resort_hotel = X[X["hotel"] == "Resort Hotel"]
    city_hotel =  X[X["hotel"] == "City Hotel"]

    cancel_reso = pd.DataFrame(resort_hotel["is_canceled"].value_counts())
    cancel_reso.rename(columns={"is_canceled": "booking_cancellations"}, index =({0: "No Canceled", 1:"Yes Canceled"}), inplace=True)
    cancel_reso["Status"] = cancel_reso.index

    cancel_city = pd.DataFrame(city_hotel["is_canceled"].value_counts())
    cancel_city.rename(columns={"is_canceled": "booking_cancellations"}, index =({0: "No Canceled", 1:"Yes Canceled"}), inplace=True)
    cancel_city["Status"] = cancel_city.index


    # countries that show the most total number of guests for each hotel
    country_guest = pd.DataFrame(X.loc[X['is_repeated_guest'] == 0]["country"].value_counts())
    country_guest.index.name = "country"
    country_guest.rename(columns={"country": "Number of Guests"}, inplace=True)
    total_num_guests = country_guest["Number of Guests"].sum()
    country_guest["% Guests"] = round(country_guest["Number of Guests"] / total_num_guests * 100, 2)

    # add new feature engineering from existing adr column
    # make a new column called total overnight cost in each hotel for guest
    X["adr_pp"] = X["adr"] / (X["adults"] + X["children"])
    room_guest = X[X["reservation_status"]==0]
    room_cost = room_guest[["hotel", "adr_pp", "reserved_room_type"]].sort_values("reserved_room_type")

     
    # replace N-Show status canceled status
    X['reservation_status'] = X['reservation_status'].replace({'No-Show':'Canced'})
    
    # Add target feature
    if 'reservation_status' in X.columns:
      X['reservation_status'] = X['reservation_status'].apply(lambda x: 0 if x =='Canceled' else 1)
      

    # drop the reservation_status from the previous dataset
    X.drop('reservation_status', axis=1, inplace=True)


    # return my wrangler data function
    return X
  

    # Loading the dataset file and read it
    url_hotel = 'https://raw.githubusercontent.com/oyrx/PHBS_MLF_2019_Project/master/data/hotel_bookings.csv'

    hotel_booked = my_wrangler_data(pd.read_csv(url_hotel))


    # treat all my dataset at the same way for training, validation, and testing
    train = my_wrangler_data(train)
    val = my_wrangler_data(val)
    test = my_wrangler_data(test)

hotel_booked['reservation_status']

"""### Choose which observations I will use to train, validate, and test my model

### Split Data
"""

# Split my feature matrix and target vector
y = hotel_booked['reservation_status']
X = hotel_booked.drop(['reservation_status'], axis=1)

# train on reviews from 2015 & earlier. Validate on 2016. Test on 2017 & later
hotel_booked['reservation_status_date'] = pd.to_datetime(hotel_booked['reservation_status_date'])
train = hotel_booked[hotel_booked['reservation_status_date'].dt.year <= 2015]
val = hotel_booked[hotel_booked['reservation_status_date'].dt.year == 2016]
test =hotel_booked[hotel_booked['reservation_status_date'].dt.year >= 2017]

# prrint out all shapes for training data, testing, and validation data
print("Training Shape:", train.shape)
print("Validation Shape:", val.shape)
print("Testing Shape:", test.shape)

"""### Begin to choose which features, if any, to exclude"""

# Selecting features
# The reservation_status column is the my target
my_target = 'reservation_status'
 
# Get a dataframe with all train columns except the target and the high cardinalities
train_features = hotel_booked.columns.drop([my_target], 'reservation_status_date')

# Get a list of the numerical features and categorical_features
numerical_features = ['total_of_special_requests', 'lead_time', 'is_repeated_guest', 'previous_cancellations','previous_bookings_not_canceled',
                      'stays_in_week_nights', 'required_car_parking_spaces']
categorical_features = ['arrival_date_month', 'deposit_type', 'reserved_room_type','hotel', 'customer_type',
 'market_segment', 'meal']
 
# Combine all the lists 
my_features = numerical_features + categorical_features

# Arrange data into features matrix and target vector 
X_train = train[my_features]
y_train = train[my_target]
X_val = val[my_features]
y_val = val[my_target]
X_test = test[my_features]
y_test = test[my_target]

print(X_train.shape)
print(y_train.shape)
print(X_val.shape)
print(y_val.shape)
print(X_test.shape)
print(y_test.shape)

# selecting some features to have in the future
my_features

"""### BaseLines
Those baselines just to help me what I can predict for my building models
"""

# initial baseline for the training
print('Baseline Training Accuracy:', y_train.value_counts(normalize=True).max())

# initial baseline for the validaton
print('Baseline Validation Accuracy:', y_val.value_counts(normalize=True).max())

# Commented out IPython magic to ensure Python compatibility.
# appropriator importance estimators to setup python enironment
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from scipy.stats import randint, uniform
import category_encoders as ce
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import f_regression, SelectKBest
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import validation_curve
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from category_encoders import OneHotEncoder, OrdinalEncoder
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.metrics.scorer import check_scoring  
from sklearn.metrics import confusion_matrix
from xgboost import XGBClassifier
from sklearn.metrics import plot_confusion_matrix, classification_report
from eli5.permutation_importance import get_score_importances
from eli5.sklearn.utils import pandas_available

"""### On One Hot Encoder"""

# one hot encoder on all the non-numerical columns
encoder = ce.OneHotEncoder(use_cat_names=True)
imputer = SimpleImputer()
scaler = StandardScaler()
Logistic_model = LogisticRegression(max_iter=1000)

X_train_encoded = encoder.fit_transform(X_train)
X_train_encoded.head()

"""### Building The Model With Decision Tree"""

tree_model = make_pipeline(ce.OrdinalEncoder(),
                       SimpleImputer(strategy='mean'), 
                       StandardScaler(),
                       DecisionTreeClassifier(max_depth=2,
                                              random_state=42,
                                              criterion='gini', 
                                              splitter='best',
                                              min_samples_split=2,
                                              min_samples_leaf=1,
                                              min_weight_fraction_leaf=0.0))

# Fitting the tree_model 
tree_model.fit(X_train, y_train);

# Check Evaluation Metrics for the tree_model
print('Traing Accuracy:', tree_model.score(X_train, y_train))
print('Validation Accuracy:', tree_model.score(X_val, y_val))
print('Testing Accuracy:', tree_model.score(X_test, y_test))

"""### Would some features “leak” future information?

### So, in order to check for leakage. First I need to graph my model to see where is the leakage.
"""

# Get the insight of the tree model
classifier = tree_model.named_steps['decisiontreeclassifier']
feature_names = tree_model.named_steps['ordinalencoder'].feature_names

# Making  a visualization for the tree
import graphviz
from sklearn.tree import export_graphviz

dot_data = export_graphviz(classifier, 
                           out_file=None, 
                           max_depth=5, 
                           feature_names=feature_names,
                           impurity=False, 
                           filled=True, 
                           proportion=True, 
                           rounded=True)   
display(graphviz.Source(dot_data))

"""### While I was processing the model, I found "leakage" future information, and it gave me a 100% accuracy. 
In fact, I could say the feature might not be useful to use in the real-world to predict results for the future without fixing the leakage. In order to check for leakage, first I needed to graph my model to see where was the leakage came from. The leakage was coming from the previous dataset's column which is-canceled column. I just forgot to drop it. I could say that I was doing something wrong, and I should fix it first and move on to the next step by dropping that column cause the leakage. After fixing that issue, I can predict better results for my model's accuracy

### Building Model With Logistic
"""

Logistic_model = make_pipeline(ce.OrdinalEncoder(),
                       SimpleImputer(strategy='mean'), 
                       StandardScaler(),
                       LogisticRegression(max_iter=100, C=1.0, random_state=42)
)

# Fitting the tree_model 
Logistic_model.fit(X_train, y_train);

# Check Evaluation Metrics for the logistic_model
print('Traing Accuracy:', Logistic_model.score(X_train, y_train))
print('Validation Accuracy:', Logistic_model.score(X_val, y_val))
print('Testing Accuracy:', Logistic_model.score(X_test, y_test))

"""### Build The Model With RandomForest"""

# Make pipeline as a one package for all
Rand_forest = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(strategy='mean'), 
    RandomForestClassifier(n_estimators=20,
                           random_state=42,
                           n_jobs=-1,
                           criterion='gini',
                           max_depth=2,
                           min_samples_split=2,
                           min_samples_leaf=1,
                           min_weight_fraction_leaf=0.0,
                           max_features='auto')
)

# Fitting the tree_model 
Rand_forest.fit(X_train, y_train);

# Check Evaluation Metrics for the Rand_forest
print('Traing Accuracy:', Rand_forest.score(X_train, y_train))
print('Validation Accuracy:', Rand_forest.score(X_val, y_val))
print('Testing Accuracy:', Rand_forest.score(X_val, y_val))

"""### Build The Model With  XGBClassifier"""

# make a model with XGBClassifier
xgb_model = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(strategy='median'), 
    XGBClassifier(n_jobs=-1,
                  early_stopping_rounds=5,
                  max_depth=2,
                  learning_rate=0.1,
                  n_estimators=20,
                  verbosity=1)
)

# fit the model
xgb_model.fit(X_train, y_train);

print('Training Accuracy:', xgb_model.score(X_train, y_train))
print('Validation Accuracy:', xgb_model.score(X_val, y_val))
print('Testing Accuracy:', xgb_model.score(X_test, y_test))

"""### Importance Features

### With RandomForest Top 10 Importance Features
"""

# Commented out IPython magic to ensure Python compatibility.
# make top 10 feature importances withe randomforst
classifier= Rand_forest.named_steps['randomforestclassifier']
f_importances = pd.Series(classifier.feature_importances_, X_train.columns)

# plot feature importances
# %matplotlib inline
import matplotlib.pyplot as plt

n = 10
plt.figure(figsize=(10,n/2))
plt.title(f'Top {n} features')
f_importances.sort_values()[-n:].plot.barh(color='green');

"""### With Decision Tree Top 8 Importance Features"""

# Importance features of my tree to get the top 8 features
import matplotlib.pyplot as plt

# Get the insight of the tree model
classifier = tree_model.named_steps['decisiontreeclassifier']

feature_names = tree_model.named_steps['ordinalencoder'].feature_names

features_importances = classifier.feature_importances_
importances = pd.Series(features_importances, index=feature_names)

n = 8
plt.figure(figsize=(8,n/2))
plt.title(f'Top {n} features')
f_importances.sort_values()[-n:].plot.barh(color='purple');

"""### Accuracy/ Precision/ Recall/ ROC-AUC Curve"""

from sklearn.metrics import plot_confusion_matrix, classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

# Predictor
y_pred_tree = tree_model.predict(X_val)

# Predictor
y_pred_log = Logistic_model.predict(X_val)

# Predictor
y_pred_rand = Rand_forest.predict(X_val)

# Predictor
y_pred_xgb = xgb_model.predict(X_val)

print("Accuracy Score For Decision Tree:", accuracy_score(y_val, y_pred_tree, normalize=True))
print("Precision Score For Decision Tree:", precision_score(y_val, y_pred_tree, pos_label=1, average='macro'))
print("Recall Score For Decision Tree:", recall_score(y_val, y_pred_tree, pos_label=1, average='binary'))

print("Accuracy Score For LogisticRegression:", accuracy_score(y_val, y_pred_log, normalize=True))
print("Precision Score For LogisticRegression:", precision_score(y_val, y_pred_log, pos_label=1, average='macro'))
print("Recall Score For LogisticRegression:", recall_score(y_val, y_pred_log, pos_label=1, average='binary'))

print("Accuracy Score For RandForest:", accuracy_score(y_val, y_pred_rand, normalize=True))
print("Precision Score For RandForest:", precision_score(y_val, y_pred_rand, pos_label=1, average='macro'))
print("Recall Score For RandForest:", recall_score(y_val, y_pred_rand, pos_label=1, average='binary'))

print("Accuracy Score For Xgboosting:", accuracy_score(y_val, y_pred_xgb, normalize=True))
print("Precision Score For Xgboosting:", precision_score(y_val, y_pred_xgb, pos_label=1, average='macro'))
print("Recall Score For Xgboosting:", recall_score(y_val, y_pred_xgb, pos_label=1, average='binary'))

# predict a probability with roc_curve for decision tree
y_predict_proba_t = tree_model.predict_proba(X_val)[:, -1]
fpr, tpr, thresholds = roc_curve(y_val, y_predict_proba_t)

# predict a probability with roc_curve for logistic
y_predict_proba_l = Logistic_model.predict_proba(X_val)[:, -1]
fpr, tpr, thresholds = roc_curve(y_val, y_predict_proba_l)

# predict a probability with roc_curve for the rand forest
y_predict_proba_r = Rand_forest.predict_proba(X_val)[:, -1]
fpr, tpr, thresholds = roc_curve(y_val, y_predict_proba_r)

# predict a probability with roc_curve for xgboosting
y_predict_proba_x = xgb_model.predict_proba(X_val)[:, -1]
fpr, tpr, thresholds = roc_curve(y_val, y_predict_proba_x)

# predict a probability of roc_curve table resualts
FP_TP_TR = pd.DataFrame({"False Positive Rate": fpr,
              "True Positive Rate": tpr,
              "Thresholds Rate": thresholds})
FP_TP_TR

from sklearn.metrics import roc_auc_score

print("Roc-Auc-Score For Tree Model:", roc_auc_score(y_val, y_predict_proba_t))
print("Roc-Auc-Score For Logistic Model:", roc_auc_score( y_val, y_predict_proba_l))
 print("Roc-Auc-Score For Random Forest Model:", roc_auc_score(y_val, y_predict_proba_r))
print("Roc-Auc-Score For XGBoosting Model:", roc_auc_score(y_val, y_predict_proba_x))

# create true and false positive rates
false_positive_rate, true_positive_rate, threshold = roc_curve(y_val, y_predict_proba_t)
false_positive_rate1, true_positive_rate1, threshold = roc_curve(y_val, y_predict_proba_l)
false_positive_rate2, true_positive_rate2, threshold = roc_curve(y_val, y_predict_proba_r)
false_positive_rate3, true_positive_rate3, threshold = roc_curve(y_val, y_predict_proba_x)

# Plot ROC curve
plt.figure(figsize=(10,6))
plt.title('Receiver Operating Characteristic For Decision Tree')
plt.plot(false_positive_rate, true_positive_rate)
plt.plot([0, 1], ls="--")
plt.plot([0, 0], [1, 0] , c=".9"), plt.plot([1, 1] , c=".9")
plt.ylabel('True Positive Rate Tree')
plt.xlabel('False Positive Rate Tree')

plt.figure(figsize=(10,6))
plt.title('Receiver Operating Characteristic For Logistic')
plt.plot(false_positive_rate1, true_positive_rate1)
plt.plot([0, 1], ls="--")
plt.plot([0, 0], [1, 0] , c=".9"), plt.plot([1, 1] , c=".9")
plt.ylabel('True Positive Rate Logistic')
plt.xlabel('False Positive Rate Logistic')

plt.figure(figsize=(10,6))
plt.title('Receiver Operating Characteristic For Random Forest')
plt.plot(false_positive_rate2, true_positive_rate2)
plt.plot([0, 1], ls="--")
plt.plot([0, 0], [1, 0] , c=".9"), plt.plot([1, 1] , c=".9")
plt.ylabel('True Positive Rate RandomForeset')
plt.xlabel('False Positive Rate RandomForeset')


plt.figure(figsize=(10,6))
plt.title('Receiver Operating Characteristic For Xgboosting')
plt.plot(false_positive_rate3, true_positive_rate3)
plt.plot([0, 1], ls="--")
plt.plot([0, 0], [1, 0] , c=".9"), plt.plot([1, 1] , c=".9")
plt.ylabel('True Positive Rate RandomForeset')
plt.xlabel('False Positive Rate RandomForeset')
plt.show()

"""### Tuning and Hyper Parametters"""

# use optimization techniques with RandomizedSearchCV
Rand_model = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(strategy='mean'), 
    RandomForestClassifier(n_estimators=20, max_depth= 2, random_state=42, n_jobs=-1)
)
# using hyperparametter for RandomForest
paramtter = {'simpleimputer__strategy':['mean', 'most_frequent', 'median'],
             'randomforestclassifier__n_estimators': [5,10,20],
             'randomforestclassifier__criterion': ['gini','entropy'],  
             'randomforestclassifier__min_samples_split': [5,10,20],
             'randomforestclassifier__max_depth': [4,10,15], 
             'randomforestclassifier__min_samples_leaf': [5,10,20]}
research_r= RandomizedSearchCV(Rand_model,
                  param_distributions=paramtter,
                  n_iter=3,
                  n_jobs=-1,
                  return_train_score=True,
                  scoring='accuracy',
                  verbose=1,
                  cv=4)
# fitting the model
research_r.fit(X_train, y_train);

best_rand_search = research_r.best_score_
best_estimator = research_r.best_params_
best_model_estimator = research_r.best_estimator_

print("Best Score:", research_r.best_score_ )
print("Best Rand Forest Score:",  research_r.best_score_)

print("Best Model:", research_r.best_estimator_)
print("Best Estimator:", research_r.best_params_)

"""### Permutation & Boosting

### Xgboost For Gradient Boosting
It is with early stopping to prevent the overfitting and underfitting, and it is optimized hyperparametters with the best estimators. Also it helps to improve the model and select feature importances.
"""

from xgboost import XGBClassifier

xgb_model = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(strategy='median'), 
    XGBClassifier(n_jobs=1, early_stopping_rounds=10, max_depth=2, learning_rate=0.1, n_estimators=20, verbosity=1)
)

# fit the model
xgb_model.fit(X_train, y_train);

print('Training Accuracy:', xgb_model.score(X_train, y_train))
print('Validation Accuracy:', xgb_model.score(X_val, y_val))
print('Testing Accuracy:', xgb_model.score(X_test, y_test))

from xgboost.sklearn import XGBClassifier
from sklearn.inspection import permutation_importance

encoder = ce.OrdinalEncoder()
X_train_encoded = encoder.fit_transform(X_train)
X_val_encoded = encoder.transform(X_val)
X_test_encoded = encoder.transform(X_test)

model_xgoost = XGBClassifier(n_jobs=-1, early_stopping_rounds=10, max_depth=2, learning_rate=0.1, n_estimators=20, verbosity=1, objective='binary:logistic', booster='gbtree',min_child_weight=1, 
                              subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, random_state=42)
evel_set = ((X_train_encoded, y_train),
(X_val_encoded, y_val), (X_test_encoded, y_test))

model_xgoost.fit(X_train_encoded, y_train)

print('Training Accuracy:', model_xgoost.score(X_train_encoded, y_train))
print('Validation Accuracy:', model_xgoost.score(X_val_encoded, y_val))
print('Testing Accuracy:', model_xgoost.score(X_test_encoded, y_test))

"""### Permutation Importance
Permutation Importance, I think it is very useful if we compromise between feature importance for reduction that it can be best or fastest.
"""

#XGBoosting with hyperparametters

paramet = {
    'criterion': 'giny', 
    'learning_rate': 0.04, 
    'max_depth': 5,
    'n_estimators': 200, 
    'objective': 'binary:logistic', 
}
#XGB model
model_xgoost= XGBClassifier(parameters=paramet)

# fit the model
model_xgoost.fit(X_train_encoded, y_train)

# perform permutation importance
xgb_result = permutation_importance(model_xgoost, X_train_encoded, y_train, scoring='accuracy', n_repeats = 10, n_jobs=-1)
xgb_in = xgb_result.importances_mean.argsort()

#XGB model
model_xgoost= XGBClassifier(parameters=paramet)

# fit the model
model_xgoost.fit(X_train_encoded, y_train)

# perform permutation importance
xgb_result = permutation_importance(model_xgoost, X_train_encoded, y_train, scoring='accuracy', n_repeats = 10, n_jobs=-1)
xgb_in = xgb_result.importances_mean.argsort()

# make table scores for features by using a for loop
for i,j in enumerate(xgb_in):
    print("Feature Scores: %0d, Score: %.5f " % (i,j))

# Xgboosting graph for permutation importance

fig, ax = plt.subplots(figsize=(10,10))

ax.boxplot(xgb_result.importances[xgb_in].T,
           vert=False, labels=X.columns[xgb_in])
ax.set_title("Xgboosting Graph For Permutation Importance")
fig.tight_layout()
plt.show()

# tree decision model
tree_model1 = make_pipeline(ce.OrdinalEncoder(),
                       SimpleImputer(strategy='mean'), 
                       StandardScaler(),
                       DecisionTreeClassifier(criterion= 'gini', min_samples_split=9,
                                  min_samples_leaf = 5, max_features = 'auto'))

# fit the model
tree_model1.fit(X_train_encoded, y_train);

#Predict Model
predict_tree = tree_model1.predict(X_test_encoded)

print("Testing Accuracy For Tree Model:", tree_model1.score(X_test_encoded, predict_tree))

# logistic model
Logistic_model1 = make_pipeline(ce.OrdinalEncoder(),
                       SimpleImputer(strategy='mean'), 
                       StandardScaler(),
                       LogisticRegression(max_iter=1000)
)

# fit the model
Logistic_model1.fit(X_train_encoded, y_train);

#Predict Model
predict_log = Logistic_model.predict(X_test_encoded)

print("Testing Accuracy For Logistic Model:", Logistic_model1.score(X_test_encoded, predict_log))

# Randforest model
Rand_forest1 = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(strategy='mean'), 
    RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1
                           , min_samples_leaf = 7, min_samples_split=8)
)

# fit the model
Rand_forest1.fit(X_train_encoded, y_train);

#Predict Model
predict_randf = Rand_forest1.predict(X_test_encoded)

print("Testing Accuracy For Random Forest:", Rand_forest1.score(X_test_encoded, predict_randf))

# extreme gradient boosting to predict the model 
model_xgoost = XGBClassifier(criterion = 'giny', learning_rate = 0.02, max_depth = 6, n_estimators = 200,
                          objective ='binary:logistic', subsample = 1.0)
# fit the model
model_xgoost.fit(X_train_encoded, y_train);

#Predict Model
predict_xgb  = model_xgoost.predict(X_test_encoded)


print("Testing Accuracy For XGBoosting Model:", model_xgoost.score(X_test_encoded, predict_xgb))

"""### Classification Reports For Models"""

# Classification Reports for all models
print("Decision Tree Model", classification_report(y_val, y_pred_tree))
print("Logistic Model",classification_report(y_val, y_pred_log))
print("Random Forest Model", classification_report(y_val, y_pred_rand))
print("XGBoosting Model", classification_report(y_val, y_pred_xgb))

"""### Confusion Matrix For Models"""

from sklearn.metrics import plot_confusion_matrix, classification_report
from sklearn.metrics import confusion_matrix

# Confusion Matrix 
tree_matrix = confusion_matrix(y_test, predict_tree)
log_matrix = confusion_matrix(y_test, predict_log)
randf_matrix = confusion_matrix(y_test, predict_randf)
xgb_matrix = confusion_matrix(y_test, predict_xgb)

"""### Plot All The Confusion Matrixes"""

# plot all the confusion matrixes for models
fig, ax = plt.subplots(1, 2, figsize=(15, 8))

sns.heatmap(tree_matrix,annot=True, fmt="d", cbar=False, cmap="Pastel2",  ax = ax[0]).set_ylim([0,2])
ax[0].set_title("Decision Tree Matrix", weight='bold')
ax[0].set_xlabel('Predicted Labels')
ax[0].set_ylabel('Actual Labels')

sns.heatmap(log_matrix ,annot=True, fmt="d" ,cbar=False, cmap="tab20", ax = ax[1]).set_ylim([0,2])
ax[1].set_title("Logistic Confusion Matrix", weight='bold')
ax[1].set_xlabel('Predicted Labels')
ax[1].set_ylabel('Actual Labels')

fig, axe = plt.subplots(1, 2, figsize=(15, 8))

sns.heatmap(randf_matrix,annot=True, fmt="d", cbar=False, cmap="Paired", ax = axe[0]).set_ylim([0,2])
axe[0].set_title("Random Forest Confusion Matrix", weight='bold')
axe[0].set_xlabel('Predicted Labels')
axe[0].set_ylabel('Actual Labels')

sns.heatmap(xgb_matrix ,annot=True, fmt="d", cbar=False, cmap="Pastel1", ax = axe[1]).set_ylim([0,2])
axe[1].set_title("Gradient Boosting Confusion Matrix", weight='bold')
axe[1].set_xlabel('Predicted Labels')
axe[1].set_ylabel('Actual Labels')

"""### Permutation Importance With Eli5"""

import eli5
from eli5.sklearn import PermutationImportance

model_predictor = Rand_forest.named_steps['randomforestclassifier']

Rand_pipeline = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(strategy='median'))

# fit the model
Rand_pipeline.fit(X_train, y_train)

# transform the model
TT_val = Rand_pipeline.transform(X_val)

model_permuter = PermutationImportance(
    model_predictor,
    scoring='accuracy',
    n_iter=7,
    random_state=42
)

model_permuter.fit(TT_val, y_val);

# eli5 graph with weight and feature with my 14 selecting features
eli5.show_weights(
    model_permuter,
    top=None,
    feature_names=X_val.columns.tolist()
)

"""### Model Interpretation

### Isolated Partial Dependence Plots with 1 feature
"""

plt.rcParams['figure.dpi']=70
pdf_feature = 'lead_time'

isolated_features = pdp_isolate(
    model=Rand_forest, 
    dataset=X_val, 
    model_features=X_val.columns, 
    feature=pdf_feature
)

pdp_plot(isolated_features, feature_name=pdf_feature);

"""### Isolated Partial Dependence Plots with 2 features"""

from pdpbox.pdp import pdp_isolate, pdp_plot

#plt.rcParams['figure.dpi']=72
pdf_feature = ['previous_bookings_not_canceled', 'previous_cancellations']

isolated_features = pdp_isolate(
    model=Rand_forest, 
    dataset=X_val, 
    model_features=X_val.columns, 
    feature=pdf_feature
)

pdp_plot(isolated_features, feature_name=pdf_feature);

"""### Interactive Partial Dependence Plots with 2 features"""

from pdpbox.pdp import pdp_interact, pdp_interact_plot

# PDF between total of special request and is repeated guest 
# it shows that the numbers of unique grid points for each 
# total of special requestis 2 and repeated guest is 4
pdf_features = ['is_repeated_guest', 'total_of_special_requests']

booking_interaction = pdp_interact(
    model=Rand_forest, 
    dataset=X_val,
    model_features=X_val.columns, 
    features=pdf_features
)

# this  multiple classes which is total of special request and is_rpeated_quest
# with numbers of grid points that 2:4
pdp_interact_plot(booking_interaction, plot_type='grid', 
                  feature_names=pdf_features);

"""### Shapley Values
It is a good technique to show the insight of the model predictor and break down each model individually.
"""

# explain the individual observation
# if I want to look for the first row from X_test
# turn it into a datafrme
first_row=X_test_encoded.iloc[[0]]
first_row

# what is the actual reservation status for the hotel booking
# by the y_test for the first row which is checkout
y_test.iloc[[0]]

# what is the model prediction for the hotel booking
research_r.predict(first_row)

# explain why the model predict checkout by using shapley values force plot
import shap

model_explain = shap.TreeExplainer(model_xgoost)
shap_values = model_explain.shap_values(X_test_encoded.iloc[1])

# make a shapely plot
shap.initjs()
shap.force_plot(
    base_value=model_explain.expected_value, shap_values=shap_values, features=first_row
    )

# fit the encoder
X_test_encoded=encoder.fit_transform(X_test, y_train)

# get an individual row  for explaination
row=X_test_encoded.iloc[[2]]
row.columns

# what is the reservation status for these features
y_test.iloc[[2]]

# model prediction for second row which is Canceled

model_xgoost.predict(row)

#using Shapley Force Plot to explain the prediction

import shap

explainer= shap.TreeExplainer(model_xgoost)
shap_values= explainer.shap_values(row)


shap.initjs()
shap.force_plot( 
    base_value=explainer.expected_value,
    shap_values=shap_values,
    features=row)

#base value 
#baseline mean
explainer.expected_value, y_train.mean()

feature_names= row.columns
feature_values=row.values[0]
shaps= pd.Series(shap_values[0], zip( feature_names, feature_values))

# calculate predicted value manually
explainer.expected_value +shaps.sum()

# make a prediction function
def predict(total_of_special_requests, lead_time, is_repeated_guest,
       previous_cancellations, previous_bookings_not_canceled,
       stays_in_week_nights, required_car_parking_spaces,
       arrival_date_month, deposit_type, reserved_room_type, hotel,
       customer_type, market_segment, meal):

  #Make dataframe from inputs
  df_book = pd.DataFrame(
      data = [[total_of_special_requests, lead_time, is_repeated_guest,
       previous_cancellations, previous_bookings_not_canceled,
       stays_in_week_nights, required_car_parking_spaces,
       arrival_date_month, deposit_type, reserved_room_type, hotel,
       customer_type, market_segment, meal]],
      columns = ['total_of_special_requests', 'lead_time', 'is_repeated_guest',
       'previous_cancellations', 'previous_bookings_not_canceled',
       'stays_in_week_nights', 'required_car_parking_spaces',
       'arrival_date_month', 'deposit_type', 'reserved_room_type', 'hotel',
       'customer_type', 'market_segment', 'meal']
      )
  
  print(df_book)
  #Get model's prediction
  pred=model_xgoost.predict(df_book)[0]

  #Calculate the shap values
  model_explainer=shap.TreeExplainer(model_xgoost)
  shap_values=model_explainer.shap_values(df_book)

  #Get series with shap values, feature names & feature values
  feature_names=df_book.columns
  feature_values=df_book.values[0]
  shaps=pd.Series(shap_values[0], zip(feature_names, feature_values))

  res = f'${pred:,.02f}'
  res +=shaps.to_string()
  print(model_explainer.expected_value)
  print(res)

  # show shapley values force plot
  shap.initjs()
  return shap.force_plot(
      base_value=model_explainer.expected_value,
      shap_values=shap_values,
      features=df_book
  )

# make a prediction
predict(30,20,10,5, 1, 2, 3,4,5,6,7,8,9,10)